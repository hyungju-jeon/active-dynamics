# Template configuration for new experiments
# Copy this file and modify parameters for your specific experiment

# @package _global_
defaults:
  - _self_
  - override hydra/launcher: joblib

# Hydra configuration
hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra:job.num}

# Experiment configuration - modify these for your experiment
seed: 42
device: "cuda"  # or "cpu", "mps"
results_dir: "results"  # Will be overridden by Hydra
action_dim: 2
observation_dim: 20
latent_dim: 2

# Environment configuration
environment:
  environment_type: "vectorfield"  # or "cartpole", "maze"
  env_dynamics_type: "limit_cycle"  # or "double_limit_cycle", "multi_attractor"
  env_noise_scale: 0.01
  env_render_mode: null  # or "human", "rgb_array"
  env_action_bounds: [-0.1, 0.1]
  env_state_bounds: null
  env_n_grid: 40
  env_w_attractor: 0.1
  env_length_scale: 0.5
  env_alpha: 0.25
  observation_type: "loglinear"  # or "linear", "identity"
  obs_hidden_dims: [16]
  obs_activation: "relu"
  noise_type: "gaussian"
  noise_scale: 0.0
  action_type: "identity"  # or "linear", "mlp"
  act_hidden_dims: [16]
  act_activation: "relu"

# Model configuration
model:
  encoder_type: "rnn"  # or "mlp"
  enc_hidden_dims: [16]
  enc_hidden_dim: 16
  enc_rnn_type: "gru"  # or "lstm"
  enc_num_layers: 1
  enc_activation: "relu"
  mapping_type: "loglinear"  # or "linear", "identity", "mlp"
  map_hidden_dims: [16]
  map_activation: "relu"
  noise_type: "gaussian"
  dynamics_type: "mlp"  # or "linear", "rbf"
  is_ensemble: false
  n_models: 1
  dyn_hidden_dims: [16]
  dyn_alpha: 0.1
  dyn_gamma: 1.0
  dyn_range: 2.0
  dyn_num_grid: 25
  action_type: "identity"
  act_hidden_dims: [16]
  act_activation: "relu"
  model_type: "seq-vae"

# Policy configuration
policy:
  policy_type: "random"  # or "lazy", "mpc-icem"
  mpc_verbose: false
  mpc_horizon: 10
  mpc_num_samples: 32
  mpc_num_iterations: 10
  mpc_num_elite: 100
  mpc_alpha: 0.1
  mpc_init_std: 0.5
  mpc_noise_beta: 1.0
  mpc_factor_decrease_num: 1.25
  mpc_frac_prev_elites: 0.2
  mpc_frac_elites_reused: 0.3
  mpc_use_mean_actions: true
  mpc_shift_elites: true
  mpc_keep_elites: true

# Metric configuration
metric:
  metric_type: ["A-optimality"]  # or ["D-optimality"], ["action"], ["goal"], ["reward"]
  compute_type: "sum"  # or "last", "max"
  gamma: 1.0
  composite_weights: null
  met_goal: null
  met_discount_factor: 0.99
  met_use_diag: false
  met_sensitivity: true
  met_covariance: "invariant"  # or "1st", "deterministic"

# Training configuration
training:
  total_steps: 10000
  train_every: 1
  save_every: 1000
  animate_every: 1000
  rollout_horizon: 20
  batch_size: 1
  learning_rate: 1e-3  # 0.001
  weight_decay: 1e-5   # 0.00001
  beta: 1.0
  n_epochs: 1
  verbose: false
  grad_clip_norm: 10.0
  n_samples: 5
  k_steps: 5
  perturbation: 0.01
  optimizer: "SGD"  # or "Adam", "AdamW"

# Logging configuration
logging:
  log_every: 100
  plot_every: 1000
  save_every: 1000
  video_path: null
  model_path: "results/models"
  buffer_path: "results/buffers"
  logger_path: "results/logs"
